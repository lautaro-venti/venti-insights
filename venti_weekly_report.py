# -*- coding: utf-8 -*-
"""
Venti ‚Äì Insight IA: Reporte semanal Intercom (Notion narrativo + Gemini API)

Fixes clave en esta versi√≥n:
- Boxplot "TTR por Urgencia (horas)" + insight (siempre con foquito debajo; con fallback sin IA).
- Insights de TODOS los gr√°ficos siempre DEBAJO de la imagen (nunca arriba).
- Sin duplicaci√≥n del insight de Top Issues.
- Removido bloque p√∫blico de "tr√≠o vago / QC" (solo m√©tricas internas por print).
- Pie de Urgencias: oculta 0% y evita superposiciones (autopct que esconde porcentajes <1%).
- Top 3 issues (bullets) y gr√°fico usan la MISMA serie de conteos para que coincidan.
"""

import os
import re
import json
import argparse
import subprocess
import shutil
import hashlib
from datetime import date
import requests
import pandas as pd
import numpy as np
import unicodedata
import time

# Matplotlib headless
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# ===================== Config heur√≠stica =====================
SILENCE_MINUTES_ASSUME_RESUELTO = 60  # si no responde el contacto tras FAR en X min y hubo acci√≥n => Resuelto

# ===================== Utilidades base =====================

EMOJI_RX = re.compile(r"[\U00010000-\U0010FFFF]", flags=re.UNICODE)

def _safe_str(v: object) -> str:
    try:
        if v is None:
            return ""
        if isinstance(v, float) and pd.isna(v):
            return ""
        s = str(v)
        if s.lower() == "nan":
            return ""
        return s
    except Exception:
        return ""

def strip_emojis(s: str) -> str:
    try:
        return EMOJI_RX.sub("", s or "")
    except Exception:
        return s or ""

def _norm_txt(s: str) -> str:
    if s is None:
        return ""
    s = str(s).strip().lower()
    s = "".join(c for c in unicodedata.normalize("NFKD", s) if not unicodedata.combining(c))
    return s

def load_csv_robusto(csv_path: str) -> pd.DataFrame:
    encodings = ["utf-8-sig", "utf-8", "cp1252", "latin-1", "utf-16", "utf-16-le", "utf-16-be"]
    seps = [",", ";", "\t", None]
    last_err = None
    for enc in encodings:
        for sep in seps:
            try:
                if sep is None:
                    df = pd.read_csv(csv_path, encoding=enc, sep=None, engine="python", dtype=str, on_bad_lines="skip")
                else:
                    df = pd.read_csv(csv_path, encoding=enc, sep=sep, engine="python", dtype=str, on_bad_lines="skip")
                if df.shape[1] >= 2:
                    return df
            except Exception as e:
                last_err = e
                continue
    try:
        return pd.read_excel(csv_path, dtype=str)
    except Exception:
        pass
    raise RuntimeError(f"No pude leer el CSV. √öltimo error: {last_err}")

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    new_cols = []
    for c in df.columns:
        cc = str(c).strip().lower()
        cc = cc.replace(" ", "_").replace("__", "_")
        cc = (cc
              .replace("√≠", "i").replace("√°", "a").replace("√©", "e")
              .replace("√≥", "o").replace("√∫", "u").replace("√±", "n"))
        new_cols.append(cc)
    df.columns = new_cols

    aliases = {
        "insight_ia": ["insight", "insightia", "insight_ia"],
        "resumen_ia": ["resumen", "resumenia", "resumen_ia"],
        "palabras_clave": ["palabras_clave","palabrasclave","keywords"],
        "canal": ["canal","channel"],
        "area": ["area","√°rea","area_","area__"],
        "tema": ["tema","topic"],
        "motivo": ["motivo","reason"],
        "submotivo": ["submotivo","sub_reason","submot"],
        "urgencia": ["urgencia","priority","severity"],
        "sentimiento": ["sentimiento","sentiment"],
        "categoria": ["categoria","categoria_","categor√≠a","categoria__"],
        "link_a_intercom": ["link_a_intercom","link_intercom","link","url_intercom","link__a__intercom"],
        "id_intercom": ["id_intercom","id","conversation_id"],
        "fecha": ["fecha","date","created_at"],
        "rol": ["rol","role"],
        # KPI crudos / timing
        "created_at": ["created_at","created","createdat"],
        "first_admin_reply_at": ["first_admin_reply_at","firstadminreplyat","first_admin_reply","first_response_at"],
        "first_contact_reply_at": ["first_contact_reply_at","firstcontactreplyat","statistics_first_contact_reply_at","first_contact_reply_created_at"],
        "closed_at": ["closed_at","closed","first_close_at","statistics_first_close_at","statistics_last_close_at"],
        "ttr_seconds": ["ttr_seconds","time_to_first_close","statistics_time_to_first_close"],
        "first_response_seconds": ["first_response_seconds","first_response_time","first_reply_seconds"],
        "status": ["status","state"],
        "csat": ["csat","rating","conversation_rating","rating_value"],
    }
    present = set(df.columns)
    for canon, options in aliases.items():
        if canon in present:
            continue
        for opt in options:
            if opt in present:
                df.rename(columns={opt: canon}, inplace=True)
                present.add(canon)
                break

    base_cols = ["resumen_ia","insight_ia","palabras_clave","canal","area","tema",
                 "motivo","submotivo","urgencia","sentimiento","categoria",
                 "link_a_intercom","id_intercom","fecha","rol"]
    for canon in base_cols:
        if canon not in df.columns:
            df[canon] = ""
    return df

# ===================== Taxonom√≠as =====================

VALID_TEMAS = {
    "eventos - user ticket","eventos - user productora","lead comercial","anuncios & notificaciones",
    "duplicado","desv√≠o a intercom","sin respuesta",
}
VALID_MOTIVOS = {
    "caso excepcional","reenv√≠o","estafa por reventa","compra externa a venti","consulta por evento",
    "team leads & p√∫blicas","devoluci√≥n","pagos","seguridad","evento reprogramado","evento cancelado",
    "contacto comercial","anuncios & notificaciones","duplicado","desv√≠o a intercom","no recib√≠ mi entrada",
    "sdu (sist. de usuarios)","transferencia de entradas","qr shield","venti swap","reporte","carga masiva",
    "env√≠o de invitaciones","carga de un evento","servicios operativos","solicitud de reembolso","adelantos",
    "liquidaciones","estado de cuenta","datos de cuenta","altas en venti","app de validaci√≥n","validadores",
    "organizaci√≥n de accesos en el evento","facturaci√≥n","sin respuesta","reclamo de usuario",
    "consulta sobre uso de la plataforma","desvinculaci√≥n de personal",
}
VALID_SUBMOTIVOS = set()

def map_to_catalog(value, catalog):
    v = _norm_txt(value)
    if not v or v in ("nan","none"):
        return "", False
    norm_catalog = {_norm_txt(x): x for x in catalog}
    if v in norm_catalog:
        return norm_catalog[v], True
    for raw in catalog:
        nraw = _norm_txt(raw)
        if v in nraw or nraw in v:
            return raw, True
    return value if isinstance(value, str) else str(value), False

def enforce_taxonomy(df: pd.DataFrame) -> pd.DataFrame:
    df["tema_norm"], df["tema_ok"] = zip(*df["tema"].map(lambda x: map_to_catalog(x, VALID_TEMAS)))
    df["motivo_norm"], df["motivo_ok"] = zip(*df["motivo"].map(lambda x: map_to_catalog(x, VALID_MOTIVOS)))
    if "submotivo" in df.columns and len(VALID_SUBMOTIVOS) > 0:
        df["submotivo_norm"], df["submotivo_ok"] = zip(*df["submotivo"].map(lambda x: map_to_catalog(x, VALID_SUBMOTIVOS)))
        df["taxonomy_flag"] = ~(df["tema_ok"] & df["motivo_ok"] & df["submotivo_ok"])
    else:
        df["submotivo_norm"] = df["submotivo"]
        df["taxonomy_flag"] = ~(df["tema_ok"] & df["motivo_ok"])
    return df

def build_text_base(row: pd.Series) -> str:
    parts = []
    for col in ["resumen_ia","insight_ia","palabras_clave","tema_norm","motivo_norm","submotivo_norm","area"]:
        val = _norm_txt(row.get(col,""))
        if val and val != "nan":
            parts.append(val)
    return " | ".join(parts)

# ----------------- CSAT/ESTADO/ISSUES -----------------

def pick_csats(row):
    for k in ["csat", "csat_ic", "csat_intercom", "csat_ia", "csat_modelo", "csat_gemini"]:
        if k in row and pd.notna(row[k]):
            try:
                v = int(float(row[k]))
                if 1 <= v <= 5:
                    return v
            except:
                pass
    return np.nan

def estado_final_from(row):
    if "estado_final" in row and pd.notna(row["estado_final"]):
        return str(row["estado_final"]).strip().capitalize()

    status = str(row.get("status","")).strip().lower()
    resumen = str(row.get("resumen_ia","") or row.get("resumen","")).strip().lower()

    if "estado final: resuelto" in resumen or "resuelto" in resumen:
        return "Resuelto"
    if "estado final: pendiente" in resumen or "pendiente" in resumen:
        return "Pendiente"
    if "sin respuesta" in resumen or "estado final: sin respuesta" in resumen:
        return "Sin respuesta"

    if status == "closed":
        return "Resuelto"
    if status in {"open","snoozed"}:
        return "Pendiente"
    return "No resuelto"

ISSUE_MAP = {
    "No recib√≠ mi entrada": "Entrega de entradas",
    "QR Shield": "QR / Validaci√≥n en acceso",
    "Pagos": "Pagos / cobros",
    "Venti Swap": "Venti Swap",
    "Servicios operativos": "App / rendimiento / bug",
    "Consulta por evento": "Informaci√≥n de evento",
    "Transferencia de entradas": "Transferencia / titularidad",
    "Devoluci√≥n": "Reembolso / devoluci√≥n",
    "Contacto Comercial": "Comercial",
    "_default": "Otros",
}
ISSUE_REGEX_RULES = [
    ("Entrega de entradas", r"(no\s*recib|reenv[i√≠]o|link\s*de\s*entrada|entrada(s)?\s*(no)?\s*llega|ticket\s*no|no\s*me\s*ll[e√©]g[o√≥])"),
    ("Transferencia / titularidad", r"(transferenc|cambio\s*de\s*titular|modificar\s*(nombre|titular)|pasar\s*entrada)"),
    ("QR / Validaci√≥n en acceso", r"\bqr\b|validaci[o√≥]n|validad(or|ores)|escane"),
    ("Pagos / cobros", r"\bpago(s)?\b|cobro|rechazad|tarjeta|mercadopago|\bmp\b|cuotas"),
    ("Reembolso / devoluci√≥n", r"reembols|devoluci[o√≥]n|refund|chargeback"),
    ("Cuenta / login / registro", r"cuenta|login|logue|registr|contrase[n√±]a|clave|verificaci[o√≥]n\s*de\s*mail|correo\s*inv[a√°]lido"),
    ("App / rendimiento / bug", r"\bapp\b|aplicaci[o√≥]n|crash|no\s*funciona|bug|error\s*(t[e√©]cnico|500|404)"),
    ("Soporte / sin respuesta / SDU", r"sin\s*respuesta|\bsdu\b|jotform|demora|espera"),
    ("Informaci√≥n de evento", r"consulta\s*por\s*evento|horario|ubicaci[o√≥]n|vip|mapa|line\s*up|ingreso|puerta"),
    ("Productores / RRPP / invitaciones", r"invitaci[o√≥]n|\brrpp\b|productor|productora|validadores|operativo"),
]

def choose_issue(motivo_final, submotivo_final, texto_base):
    m = _safe_str(motivo_final).strip()
    if m in ISSUE_MAP:
        return ISSUE_MAP[m]
    sm = _safe_str(submotivo_final).strip().lower()
    if "qr" in sm:
        return "QR / Validaci√≥n en acceso"
    if "pago" in sm or "mp" in sm:
        return "Pagos / cobros"
    t = _norm_txt(_safe_str(texto_base))
    for label, rx in ISSUE_REGEX_RULES:
        if re.search(rx, t):
            return label
    return ISSUE_MAP["_default"]

# ===================== Heur√≠sticas de Issues =====================

RULES = [
    ("Entrega de entradas", r"(no\s*recibi|reenvio|link\s*de\s*entrada|entrada(s)?\s*(no)?\s*llega|ticket\s*no\s|no\s*me\s*llego)"),
    ("Transferencia / titularidad", r"(transferenc|transferir|cambio\s*de\s*titular|modificar\s*(nombre|titular)|pasar\s*entrada)"),
    ("QR / Validaci√≥n en acceso", r"(qr|validacion|control\s*de\s*acceso|escaneo|lector|validador)"),
    ("Pagos / cobros", r"(pago|pagos|cobro|cobrar|rechazad|tarjeta|mercadopago|\bmp\b|cuotas)"),
    ("Reembolso / devoluci√≥n", r"(reembolso|devolucion|refund|chargeback)"),
    ("Cuenta / login / registro", r"(cuenta|logue|login|registr|contrasena|clave|verificacion\s*de\s*mail|correo\s*invalido)"),
    ("App / rendimiento / bug", r"(app|aplicacion|crash|se\s*cierra|no\s*funciona|bug|error\s*(tecnico|500|404))"),
    ("Soporte / sin respuesta / SDU", r"(sin\s*respuesta|derivacion\s*al\s*sdu|\bsdu\b|jotform|demora|espera)"),
    ("Informaci√≥n de evento", r"(consulta\s*por\s*evento|horario|ubicacion|vip|mapa|line\s*up|capacidad|ingreso|puerta)"),
    ("Productores / RRPP / invitaciones", r"(invitacion|\brrpp\b|productor|productora|acceso\s*productor|carga\s*de\s*evento|validadores|operativo)"),
]

def assign_issue_group(text: str) -> str:
    t = _norm_txt(text)
    for label, pattern in RULES:
        if re.search(pattern, t):
            return label
    return "Otros"

def top_values(series: pd.Series, n=3) -> str:
    vc = series.fillna("").replace("nan","").astype(str).str.strip().value_counts()
    items = [f"{idx} ({cnt})" for idx, cnt in vc.head(n).items() if idx]
    return ", ".join(items)

def _safe_lower(x):
    try:
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return ""
        return str(x).strip().lower()
    except Exception:
        return ""

def compute_flags(row: pd.Series) -> pd.Series:
    txt = " ".join(str(row.get(c,"") or "") for c in
                   ["resumen_ia","insight_ia","palabras_clave","tema_norm","motivo_norm","submotivo_norm"]).lower()
    urg = _safe_lower(row.get("urgencia",""))
    canal = _safe_lower(row.get("canal",""))
    risk = "LOW"
    if any(k in _norm_txt(txt) for k in ["estafa","fraude","no puedo entrar","no puedo ingresar","rechazad"]):
        risk = "HIGH"
    elif urg in ("alta","high"):
        risk = "HIGH"
    elif urg in ("media","medium"):
        risk = "MEDIUM"
    sla_now = (canal in ("whatsapp","instagram")) and (risk == "HIGH")
    return pd.Series({"risk": risk, "sla_now": sla_now})

# ===================== Enrichment / reglas =====================

def _is_vague_summary(s: str) -> bool:
    ss = (_norm_txt(s) or "")
    return (not ss) or ("caso sin resumen explicito" in ss) or (len(ss) < 30)

def _is_vague_insight(s: str) -> bool:
    ss = (_norm_txt(s) or "")
    return (not ss) or ("registrar el caso y monitorear patrones" in ss) or (len(ss) < 30)

def _enrich_summary(row) -> str:
    canal = _safe_str(row.get("canal") or "-")
    rol   = _safe_str(row.get("rol") or "-")
    tema  = _safe_str(row.get("tema_norm") or row.get("tema") or "")
    mot   = _safe_str(row.get("motivo_norm") or row.get("motivo") or "")
    sub   = _safe_str(row.get("submotivo_norm") or row.get("submotivo") or "")
    estado= _safe_str(row.get("estado_final") or "")
    tbase = _norm_txt(row.get("texto_base",""))
    action = "seguimiento"
    if any(w in tbase for w in ["se envio","reenvio","informamos","adjuntamos","validacion","corregido","listo"]):
        action = "info enviada"
    pedido = "qu√© pidi√≥ o report√≥ el usuario"
    if "correo invalido" in tbase or "registro" in tbase:
        pedido = "problema para crear cuenta/validar email"
    elif "no recibi mi entrada" in tbase or "entrada no" in tbase:
        pedido = "no recibi√≥ su entrada y pide reenv√≠o"
    causa = ""
    if "rechazad" in tbase:
        causa = "posible pago rechazado"
    elif "qr" in tbase:
        causa = "problema de validaci√≥n QR"
    base = f"{canal}/{rol}: {pedido}"
    if mot or sub or tema:
        base += f"; motivo {mot or tema}{' / ' + sub if sub else ''}"
    if causa:
        base += f"; causa probable: {causa}"
    if estado:
        base += f"; ESTADO FINAL {estado}"
    base += f"; acci√≥n: {action}."
    return base

def _enrich_insight(row) -> str:
    issue = _safe_str(row.get("issue_group") or "Caso")
    sent = _safe_lower(row.get("sentimiento", ""))
    urg  = _safe_lower(row.get("urgencia", ""))
    kpi  = "CSAT" if sent == "negativo" else "TTR"
    prio = "Alta" if urg == "alta" else "Media"
    recs  = {
        "QR / Validaci√≥n en acceso": "Ingenier√≠a: health-checks y telemetr√≠a de validadores | CX: gu√≠a de acceso",
        "Pagos / cobros": "Producto: mostrar causa de rechazo | Tech: retries PSP | CX: macro paso a paso",
        "Entrega de entradas": "Producto: CTA reenv√≠o visible | Tech: job idempotente | CX: bot reenv√≠o",
    }
    extra = recs.get(issue, "Producto: quick wins de UX | Tech: logging/tracing | CX: macro espec√≠fica")
    return f"{issue}: priorizar fixes. {extra} ‚Äî Prioridad: {prio}; M√©trica: {kpi}; Tiempo: 2‚Äì3 semanas."

def _sent_rule(text: str) -> str:
    t = _norm_txt(text)
    if any(w in t for w in ["queja","estafa","fraude","no puedo","rechazad","error","fallo","no funciona"]): return "Negativo"
    if any(w in t for w in ["gracias","excelente","solucionado","todo ok","genial"]): return "Positivo"
    return "Neutro"

def _urg_rule(text: str) -> str:
    t = _norm_txt(text)
    if any(w in t for w in ["hoy","ya","urgente","evento","no puedo ingresar","no puedo entrar","qr","validador","no recibi mi entrada"]): return "Alta"
    if any(w in t for w in ["consulta","informacion","como hago","quiero saber"]): return "Baja"
    return "Media"

def _csat_rule(sent:str, estado:str) -> int:
    s = (sent or "").lower(); e = (estado or "").lower()
    if e == "no resuelto" or s == "negativo": return 1 if e == "no resuelto" else 2
    if e == "resuelto": return 4 if s != "negativo" else 3
    if e == "pendiente": return 2 if s == "negativo" else 3
    return 3

def _assume_resuelto_por_silencio(row) -> str:
    estado = _safe_str(row.get("estado_final",""))
    if estado in ("Resuelto","No resuelto","Sin respuesta"):
        return estado
    try:
        far = float(row.get("first_admin_reply_at", "nan"))
        fcr = float(row.get("first_contact_reply_at", "nan"))
    except Exception:
        return estado or "Pendiente"
    if np.isnan(far):
        return estado or "Pendiente"
    if np.isnan(fcr) or (fcr - far) > (SILENCE_MINUTES_ASSUME_RESUELTO * 60):
        if any(w in _norm_txt(row.get("resumen_qc","")) for w in ["reenvio","validacion","corregido","listo","informamos","enviado"]):
            return "Resuelto"
    return estado or "Pendiente"

# ===================== KPI desde campos crudos =====================

def _to_num(df, col):
    return pd.to_numeric(df[col], errors="coerce") if col in df.columns else pd.Series([np.nan]*len(df))

def compute_kpis_from_raw_df(df: pd.DataFrame, sla_minutes: int = 15) -> dict:
    created  = _to_num(df, "created_at")
    closed   = _to_num(df, "closed_at")
    far      = _to_num(df, "first_admin_reply_at")
    fcr      = _to_num(df, "first_contact_reply_at")
    frs      = _to_num(df, "first_response_seconds")
    ttr_secs = _to_num(df, "ttr_seconds")

    need_frs = frs.isna()
    base_first = far.where(~far.isna(), fcr)
    est_frs = base_first - created
    frs = frs.where(~need_frs, est_frs)
    frs = frs.where(frs >= 0, np.nan)

    need_ttr = ttr_secs.isna()
    est_ttr = closed - created
    ttr_secs = ttr_secs.where(~need_ttr, est_ttr)
    ttr_secs = ttr_secs.where(ttr_secs >= 0, np.nan)

    # Guardamos los estimados √∫tiles de vuelta (para gr√°ficos posteriores)
    df["__frs_seconds__"] = frs
    df["__ttr_seconds__"] = ttr_secs

    if "status" in df.columns:
        tickets = int((df["status"].astype(str).str.lower() == "closed").sum()) or int(len(df))
    else:
        tickets = int(len(df))

    base = int(frs.notna().sum())
    ok = int((frs <= sla_minutes*60).sum()) if base else 0
    tasa_1ra = (ok/base*100.0) if base else None

    fr_p50 = float(np.nanmedian(frs)) if base else None

    if ttr_secs.notna().any():
        ttr_mean_h = float(np.nanmean(ttr_secs))/3600.0
        ttr_p50_h  = float(np.nanpercentile(ttr_secs.dropna(), 50))/3600.0
        ttr_p90_h  = float(np.nanpercentile(ttr_secs.dropna(), 90))/3600.0
    else:
        ttr_mean_h = ttr_p50_h = ttr_p90_h = None

    csat = None; csat_n = 0
    if "csat" in df.columns:
        cs = pd.to_numeric(df["csat"], errors="coerce")
        csat_n = int(cs.notna().sum())
        if cs.notna().any():
            csat = round(float(cs.mean()), 2)

    return {
        "tickets_resueltos": tickets,
        "tasa_1ra_respuesta": float(tasa_1ra) if isinstance(tasa_1ra, (int,float)) else None,
        "first_base": base,
        "first_ok": ok,
        "first_resp_p50_s": float(fr_p50) if isinstance(fr_p50, float) else None,
        "ttr_horas": float(ttr_mean_h) if isinstance(ttr_mean_h, float) else None,
        "ttr_p50_h": float(ttr_p50_h) if isinstance(ttr_p50_h, float) else None,
        "ttr_p90_h": float(ttr_p90_h) if isinstance(ttr_p90_h, float) else None,
        "csat": csat,
        "csat_n": csat_n,
        "sla_minutes": sla_minutes,
    }

def _fmt_min_from_sec(s):
    if s is None: return "‚Äî"
    return f"{int(round(s/60.0))} min"

def _fmt_h(h):
    if h is None: return "‚Äî"
    return f"{h:.2f} h"

# ===================== Gr√°ficos =====================

def _autopct_hide_small(pct):
    return ("" if pct < 1 else f"{pct:.0f}%")

def chart_top_issues(data, out_dir, filename="top_issues.png"):
    """
    Acepta:
      - resumen_df con columnas ['issue','casos'], o
      - df crudo con columna 'issue_group'
    """
    if isinstance(data, pd.DataFrame) and {"issue","casos"}.issubset(data.columns):
        counts = (data
                  .sort_values("casos", ascending=False)
                  .set_index("issue")["casos"]
                  .head(5))
    else:
        counts = data["issue_group"].value_counts().head(5)

    fig, ax = plt.subplots(figsize=(8,5))
    labels = list(counts.index)
    vals = list(counts.values)
    y = np.arange(len(labels))
    bars = ax.barh(y, vals)
    ax.set_yticks(y); ax.set_yticklabels(labels)
    ax.set_title("Top Issues"); ax.set_xlabel("Casos")
    ax.invert_yaxis()

    # Etiquetas con valor
    for i, b in enumerate(bars):
        w = b.get_width()
        ax.text(w + (max(vals)*0.01 if vals else 0.3),
                b.get_y()+b.get_height()/2,
                f"{vals[i]}", va="center")

    p = os.path.join(out_dir, filename)
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    return p, counts


def chart_urgencia_pie(df, out_dir):
    counts = df["urgencia"].fillna("Sin dato").replace({"nan":"Sin dato"}).value_counts()
    counts = counts[counts > 0]  # fuera categor√≠as en cero
    labels = list(counts.index); vals = list(counts.values)
    fig, ax = plt.subplots(figsize=(6,6))
    if len(vals) == 0:
        vals = [1]; labels = ["Sin datos"]
    ax.pie(vals, labels=labels, autopct=_autopct_hide_small, startangle=90, pctdistance=0.75)
    ax.axis("equal")
    ax.set_title("Distribuci√≥n de Urgencias")
    p = os.path.join(out_dir, "urgencia_pie.png")
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    return p, counts

def chart_sentimiento_pie(df, out_dir):
    counts = df["sentimiento"].fillna("Sin dato").replace({"nan":"Sin dato"}).value_counts()
    counts = counts[counts > 0]
    labels = list(counts.index); vals = list(counts.values)
    fig, ax = plt.subplots(figsize=(6,6))
    if len(vals) == 0:
        vals = [1]; labels = ["Sin datos"]
    ax.pie(vals, labels=labels, autopct=_autopct_hide_small, startangle=90, pctdistance=0.75)
    ax.axis("equal")
    ax.set_title("Distribuci√≥n de Sentimientos")
    p = os.path.join(out_dir, "sentimiento_pie.png")
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    return p, counts

def chart_urgencia_por_issue(df, out_dir):
    ct = pd.crosstab(df["issue_group"], df["urgencia"]).fillna(0).astype(int)
    fig, ax = plt.subplots(figsize=(9,6))
    x = np.arange(len(ct.index))
    bottom = np.zeros(len(ct.index))
    for col in ct.columns:
        vals = ct[col].values
        ax.bar(x, vals, bottom=bottom, label=str(col))
        bottom = bottom + vals
    ax.set_xticks(x); ax.set_xticklabels(list(ct.index), rotation=45, ha="right")
    ax.set_title("Urgencia por Issue"); ax.set_ylabel("Casos"); ax.legend()
    p = os.path.join(out_dir, "urgencia_por_issue.png")
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    return p, ct

def chart_canal_por_issue(df, out_dir):
    ct = pd.crosstab(df["issue_group"], df["canal"]).fillna(0).astype(int)
    fig, ax = plt.subplots(figsize=(9,6))
    x = np.arange(len(ct.index))
    bottom = np.zeros(len(ct.index))
    for col in ct.columns:
        vals = ct[col].values
        ax.bar(x, vals, bottom=bottom, label=str(col))
        bottom = bottom + vals
    ax.set_xticks(x); ax.set_xticklabels(list(ct.index), rotation=45, ha="right")
    ax.set_title("Canal por Issue"); ax.set_ylabel("Casos"); ax.legend(ncols=2, fontsize=8)
    p = os.path.join(out_dir, "canal_por_issue.png")
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    return p, ct

def chart_urgencias_en_top_issues(df, out_dir, top_n=5):
    top = df["issue_group"].value_counts().head(top_n).index.tolist()
    sub = df[df["issue_group"].isin(top)].copy()
    urg_levels = ["Alta","Media","Baja"]
    sub["urgencia_norm"] = sub["urgencia"].fillna("Sin dato").replace({"nan":"Sin dato"}).str.title()
    sub["urgencia_norm"] = sub["urgencia_norm"].replace({"High":"Alta","Medium":"Media","Low":"Baja"})
    ct = pd.crosstab(sub["issue_group"], sub["urgencia_norm"]).reindex(columns=urg_levels, fill_value=0)
    idx = np.arange(len(ct.index))
    width = 0.25
    fig, ax = plt.subplots(figsize=(9,6))
    for i, urg in enumerate(urg_levels):
        vals = ct[urg].values if urg in ct.columns else np.zeros(len(ct.index))
        ax.bar(idx + (i-1)*width, vals, width=width, label=urg)
    ax.set_xticks(idx); ax.set_xticklabels(list(ct.index), rotation=45, ha="right")
    ax.set_title("Distribuci√≥n de Urgencias en Top Issues"); ax.set_ylabel("Casos"); ax.legend()
    p = os.path.join(out_dir, "urgencia_top_issues.png")
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    return p, ct

def chart_ttr_box_by_urgencia(df, out_dir):
    # TTR en horas desde columna calculada/estimada
    ttr = pd.to_numeric(df.get("__ttr_seconds__", df.get("ttr_seconds", np.nan)), errors="coerce")/3600.0
    urg = df["urgencia"].astype(str).str.title().replace({"High":"Alta","Medium":"Media","Low":"Baja"})
    sub = pd.DataFrame({"ttr_h": ttr, "urg": urg}).dropna()
    sub = sub[(sub["ttr_h"] >= 0) & (sub["ttr_h"] < 96)]  # outliers extremos fuera de 4 d√≠as
    order = ["Alta","Media","Baja"]
    fig, ax = plt.subplots(figsize=(9,5))
    data = [sub.loc[sub["urg"] == u, "ttr_h"].values for u in order]
    ax.boxplot(data, labels=order, showfliers=True)
    ax.set_title("TTR por Urgencia (horas)")
    ax.set_ylabel("Horas")
    p = os.path.join(out_dir, "ttr_por_urgencia_box.png")
    plt.tight_layout(); fig.savefig(p); plt.close(fig)
    # stats para insight/IA
    stats = {}
    for u in order:
        vals = sub.loc[sub["urg"] == u, "ttr_h"].dropna().values
        if len(vals):
            stats[u] = {"n": int(len(vals)), "p50": float(np.percentile(vals,50)), "p90": float(np.percentile(vals,90))}
    return p, stats

# ===================== Comparativa WoW =====================

def compare_with_prev(issues_df: pd.DataFrame, hist_dir="./hist") -> pd.DataFrame:
    os.makedirs(hist_dir, exist_ok=True)
    today = date.today().isoformat()
    issues_df.to_csv(os.path.join(hist_dir, f"issues_{today}.csv"), index=False, encoding="utf-8")

    prevs = sorted([p for p in os.listdir(hist_dir) if p.startswith("issues_") and p.endswith(".csv")])
    if len(prevs) < 2:
        issues_df["wow_change_pct"] = ""
        issues_df["anomaly_flag"] = False
        return issues_df

    prev = pd.read_csv(os.path.join(hist_dir, prevs[-2]))
    prev_map = dict(zip(prev["issue"], prev["casos"]))

    def calc(issue, casos):
        p = prev_map.get(issue, 0)
        if p == 0:
            return 100.0, True if casos >= 10 else False
        delta = (casos - p) / max(p, 1) * 100.0
        return round(delta, 1), (delta >= 50.0 and casos >= 10)

    issues_df["wow_change_pct"], issues_df["anomaly_flag"] = zip(*issues_df.apply(lambda r: calc(r["issue"], r["casos"]), axis=1))
    return issues_df

# ===================== Gemini (API REST) + presupuesto/cache =====================

class _AIBudget:
    def __init__(self, budget:int):
        self.budget = max(0, int(budget))
        self.rate_limited = False
    def take(self) -> bool:
        if self.rate_limited or self.budget <= 0:
            return False
        self.budget -= 1
        return True

def _dataset_fingerprint(df: pd.DataFrame) -> str:
    h = hashlib.sha256()
    h.update(str(len(df)).encode())
    counts = df["issue_group"].value_counts().to_dict() if "issue_group" in df.columns else {}
    h.update(json.dumps(counts, sort_keys=True).encode())
    if "created_at" in df.columns:
        try:
            h.update(str(pd.to_numeric(df["created_at"], errors="coerce").sum()).encode())
        except Exception:
            pass
    return h.hexdigest()[:16]

def _load_ai_cache(cache_path: str) -> dict:
    try:
        if os.path.exists(cache_path):
            with open(cache_path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return {}

def _save_ai_cache(cache_path: str, cache_obj: dict):
    try:
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache_obj, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

def gemini_generate_text(prompt, api_key=None, model="gemini-1.5-flash",
                         temperature=0.3, max_output_tokens=256,
                         retries=4, backoff=2.0):
    api_key = api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("‚ö†Ô∏è GEMINI_API_KEY/GOOGLE_API_KEY no seteado. Saltando.")
        return ""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    data = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": temperature, "maxOutputTokens": max_output_tokens}
    }
    for attempt in range(retries):
        try:
            r = requests.post(url, headers=headers, data=json.dumps(data), timeout=40)
            if r.status_code == 429:
                print("‚ùå Gemini 429 (rate limit).")
                return "__RATE_LIMIT__"
            if r.status_code == 503:
                wait = backoff ** attempt
                print(f"‚ùå Gemini 503 (overloaded). Retry {attempt+1}/{retries} en {wait:.1f}s")
                time.sleep(wait)
                continue
            if not r.ok:
                print(f"‚ùå Gemini HTTP {r.status_code}. Body: {r.text[:400]}")
                return "__UNAVAILABLE__"
            out = r.json()
            cand = (out.get("candidates") or [{}])[0]
            parts = ((cand.get("content") or {}).get("parts") or [{}])
            return parts[0].get("text", "").strip()
        except requests.Timeout:
            print("‚è±Ô∏è Timeout Gemini.")
        except Exception as e:
            print(f"‚ùå Excepci√≥n Gemini: {e}")
    return "__UNAVAILABLE__"

def _gemini_smoke_test(api_key: str | None, model: str) -> None:
    if not (api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")):
        print("‚ÑπÔ∏è Gemini no configurado (sin API key). Insights deshabilitados.")
        return
    txt = gemini_generate_text("Ping: responde 'OK' si recibiste este mensaje.", api_key=api_key, model=model, max_output_tokens=8)
    if txt:
        print(f"‚úÖ Gemini listo. Respuesta: {txt[:50]}")
    else:
        print("‚ö†Ô∏è Gemini no respondi√≥. Verifica key, cuotas o permisos.")

def ai_insight_for_chart(chart_name: str, stats_obj, api_key: str | None = None, model: str = "gemini-1.5-flash") -> str:
    if isinstance(stats_obj, pd.DataFrame):
        snap = stats_obj.head(10).to_string()
    else:
        try:
            snap = json.dumps(stats_obj, ensure_ascii=False)[:4000]
        except Exception:
            snap = str(stats_obj)[:4000]
    prompt = (
        f"Eres un analista de Customer Experience en una empresa de tickets. "
        f"Analiza el gr√°fico '{chart_name}'. "
        f"Identifica el hallazgo m√°s relevante y su implicancia operativa. "
        f"2 frases m√°ximo, espa√±ol, tono ejecutivo. Datos:\n{snap}\n"
        "Formato: observaci√≥n concreta + recomendaci√≥n."
    )
    return gemini_generate_text(prompt, api_key=api_key, model=model, max_output_tokens=200)

def ai_actions_for_issue(issue: str, contexto: dict, api_key: str | None = None, model: str = "gemini-1.5-flash") -> dict:
    ctx_json = json.dumps(contexto, ensure_ascii=False)
    prompt = (
        f"Eres PM/Analyst en una empresa de tickets. "
        f"Prop√≥n 1-3 acciones por categor√≠a para '{issue}': Producto, Tech y CX. "
        "Bullets concretos (‚â§14 palabras). "
        f"Contexto: {ctx_json}\n"
        "Devuelve SOLO JSON v√°lido con claves 'Producto','Tech','CX'."
    )
    txt = gemini_generate_text(prompt, api_key=api_key, model=model, max_output_tokens=320)
    if txt == "__RATE_LIMIT__":  # sin ruido en reportes
        return {"__rate_limited__": True}
    try:
        obj = json.loads(txt)
        out = {}
        for k in ["Producto","Tech","CX"]:
            if isinstance(obj.get(k), list):
                out[k] = [str(a) for a in obj[k]][:3]
        return out
    except Exception:
        return {}

# ===================== Publicaci√≥n en GitHub (assets) =====================

def _run_git(cmd, cwd):
    p = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)
    if p.returncode != 0:
        raise RuntimeError(p.stderr or p.stdout)
    return (p.stdout or "").strip()

def _parse_remote_origin(remote_url: str):
    u = remote_url.strip()
    if u.endswith(".git"):
        u = u[:-4]
    if "github.com/" in u:
        owner_repo = u.split("github.com/")[-1]
    elif "github.com:" in u:
        owner_repo = u.split("github.com:")[-1]
    else:
        raise ValueError(f"No pude parsear remote.origin.url: {remote_url}")
    parts = u.split("/")
    if len(parts) >= 2:
        return parts[-2], parts[-1]
    raise ValueError(f"URL inesperada: {remote_url}")

def publish_images_to_github(out_dir: str,
                             repo_path: str,
                             branch: str = "main",
                             date_subdir: str | None = None,
                             files: list[str] | None = None) -> str:
    if files is None:
        files = ["top_issues.png", "urgencia_pie.png", "sentimiento_pie.png",
                 "urgencia_por_issue.png", "canal_por_issue.png", "urgencia_top_issues.png",
                 "ttr_por_urgencia_box.png"]
    if date_subdir is None:
        date_subdir = date.today().isoformat()

    if not os.path.isdir(repo_path):
        raise RuntimeError(f"Repo path no existe: {repo_path}")
    for fn in files:
        p = os.path.join(out_dir, fn)
        if not os.path.exists(p):
            raise RuntimeError(f"No existe imagen: {p}")

    dest_dir = os.path.join(repo_path, "reports", date_subdir)
    os.makedirs(dest_dir, exist_ok=True)
    for fn in files:
        shutil.copy2(os.path.join(out_dir, fn), os.path.join(dest_dir, fn))

    _run_git(["git", "add", "."], cwd=repo_path)
    try:
        _run_git(["git", "commit", "-m", f"Report {date_subdir}: PNG charts"], cwd=repo_path)
    except RuntimeError as e:
        if "nothing to commit" not in str(e).lower():
            raise
    _run_git(["git", "push", "origin", branch], cwd=repo_path)

    remote = _run_git(["git", "config", "--get", "remote.origin.url"], cwd=repo_path)
    owner, repo = _parse_remote_origin(remote)
    base_raw = f"https://raw.githubusercontent.com/{owner}/{repo}/{branch}/reports/{date_subdir}"
    return base_raw

# ===================== Notion helpers =====================

def _h1(text): return {"object":"block","type":"heading_1","heading_1":{"rich_text":[{"type":"text","text":{"content":text}}]}}
def _h2(text): return {"object":"block","type":"heading_2","heading_2":{"rich_text":[{"type":"text","text":{"content":text}}]}}
def _h3(text): return {"object":"block","type":"heading_3","heading_3":{"rich_text":[{"type":"text","text":{"content":text}}]}}
def _para(text): return {"object":"block","type":"paragraph","paragraph":{"rich_text":[{"type":"text","text":{"content":text}}]}}
def _bullet(text): return {"object":"block","type":"bulleted_list_item","bulleted_list_item":{"rich_text":[{"type":"text","text":{"content":text}}]}}
def _todo(text, checked=False): return {"object":"block","type":"to_do","to_do":{"rich_text":[{"type":"text","text":{"content":text}}],"checked":checked}}
def _callout(text, icon="üí°"): return {"object":"block","type":"callout","callout":{"rich_text":[{"type":"text","text":{"content":text}}],"icon":{"type":"emoji","emoji":icon}}}
def _divider(): return {"object":"block","type":"divider","divider":{}}
def _rt(text: str): return [{"type": "text", "text": {"content": str(text)}}]

URL_RX = re.compile(r'^https?://[^\s<>"\'\|\)\]]+$', re.IGNORECASE)

def _clean_url(u: str) -> str | None:
    if not isinstance(u, str): return None
    u = u.strip().replace("\n", " ").replace("\r", " ")
    if not u: return None
    u = u.split()[0].strip('\'"()[]')
    u = ''.join(ch for ch in u if 31 < ord(ch) < 127 and ch not in {'|'})
    if not (u.lower().startswith("http://") or u.lower().startswith("https://")):
        return None
    return u if URL_RX.match(u) else None

def _link(text: str, url: str):
    safe = _clean_url(url)
    if not safe:
        return {"type":"text","text":{"content":str(text)}}
    return {"type":"text","text":{"content":str(text),"link":{"url":safe}}}

def _image_external_if_valid(url: str | None, caption: str | None = None):
    safe = _clean_url(url) if url else None
    if not safe:
        return None
    b = {"object":"block","type":"image","image":{"type":"external","external":{"url":safe}}}
    if caption:
        b["image"]["caption"] = [{"type":"text","text":{"content":caption}}]
    return b

def _column_list(columns_children: list[list[dict]]):
    return {"object":"block","type":"column_list","column_list":{"children":[{"object":"block","type":"column","column":{"children":ch}} for ch in columns_children]}}

def _notion_table(headers: list[str], rows: list[list[list[dict]]]):
    table = {"object":"block","type":"table","table":{"table_width": len(headers),"has_column_header": True,"has_row_header": False,"children":[]}}
    table["table"]["children"].append({"object":"block","type":"table_row","table_row":{"cells":[_rt(h) for h in headers]}})
    for row in rows:
        table["table"]["children"].append({"object":"block","type":"table_row","table_row":{"cells": row}})
    return table

def build_issues_table_block(resumen_df: pd.DataFrame) -> dict:
    headers = ["Issue","Casos","Canales","Areas","Motivos","Submotivos","Ejemplos"]
    rows = []
    for _, r in resumen_df.sort_values("casos", ascending=False).iterrows():
        ej_text = str(r.get("ejemplos_intercom","") or "")
        parts = [p.strip() for p in ej_text.replace("\n"," ").split("|") if p.strip()]
        urls = []
        seen = set()
        for p in parts:
            u = _clean_url(p)
            if u and u not in seen:
                urls.append(u); seen.add(u)
            if len(urls) >= 3:
                break
        examples_rt = [{"type":"text","text":{"content":"‚Äî"}}] if not urls else sum(
            [[_link(f"Ejemplo {i}", u)] + ([{"type":"text","text":{"content":"  ‚Ä¢  "}}] if i < len(urls) else []) for i,u in enumerate(urls,1)], [])
        row_cells = [
            _rt(r.get("issue","")),
            _rt(str(int(r.get("casos",0) or 0))),
            _rt(r.get("canales_top","")),
            _rt(r.get("areas_top","")),
            _rt(r.get("motivos_top","")),
            _rt(r.get("submotivos_top","")),
            examples_rt
        ]
        rows.append(row_cells)
    return _notion_table(headers, rows)

ACTION_LIBRARY = {
    "Pagos / cobros": {
        "Producto": ["Mostrar causa de rechazo y reintentos guiados.","Guardar tarjeta segura (1-click)."],
        "Tech": ["Logs de PSP + alertas por BIN/issuer.","Retries con backoff en fallas de red."],
        "CX": ["Macro paso a paso por medio de pago.","Comunicar reserva temporal."]
    },
    "Entrega de entradas": {
        "Producto": ["CTA visible para reenv√≠o y confirmaci√≥n en UI."],
        "Tech": ["Job idempotente de reenv√≠o.","Monitoreo de bounce/spam."],
        "CX": ["Bot autogestivo de reenv√≠o por mail/WhatsApp."]
    },
    "QR / Validaci√≥n en acceso": {
        "Producto": ["Feedback claro de estado del QR (v√°lido/usado/bloqueado)."],
        "Tech": ["Telemetr√≠a de validadores + health checks."],
        "CX": ["Gu√≠a de acceso y resoluci√≥n de errores comunes."]
    },
    "Transferencia / titularidad": {
        "Producto": ["Flujo guiado de cambio de titularidad con confirmaci√≥n."],
        "Tech": ["Auditor√≠a/registro de transferencias."],
        "CX": ["Macro con costos/plazos y l√≠mites."]
    },
    "Reembolso / devoluci√≥n": {
        "Producto": ["Estado visible del reembolso y tiempos estimados."],
        "Tech": ["Idempotencia + conciliaci√≥n con PSP."],
        "CX": ["Macro de seguimiento y expectativas."]
    }
}

def actions_for_issue(issue: str):
    return ACTION_LIBRARY.get(issue, {
        "Producto": ["Quick wins de UX para reducir fricci√≥n."],
        "Tech": ["Registrar error types + tracing/dashboards."],
        "CX": ["Macro de contenci√≥n + FAQ espec√≠fica."]
    })

def build_actions_section_blocks(resumen_df: pd.DataFrame, top_n: int = 5, acciones_ai: dict | None = None) -> list[dict]:
    blocks = []
    blocks.append(_h2("Acciones A Evaluar"))
    for _, r in resumen_df.sort_values("casos", ascending=False).head(top_n).iterrows():
        issue = str(r.get("issue",""))
        casos = int(r.get("casos",0) or 0)
        blocks.append(_h3(f"{issue} ({casos})"))
        base = actions_for_issue(issue)
        ai_extra = acciones_ai.get(issue, {}) if acciones_ai else {}
        col_prod = [ _para("Producto:") ] + [ _todo(a) for a in base.get("Producto", []) + ai_extra.get("Producto", []) ]
        col_tech = [ _para("Tech:") ]     + [ _todo(a) for a in base.get("Tech", []) + ai_extra.get("Tech", []) ]
        col_cx   = [ _para("CX:") ]       + [ _todo(a) for a in base.get("CX", []) + ai_extra.get("CX", []) ]
        blocks.append(_column_list([col_prod, col_tech, col_cx]))
    return blocks

# -------- sanitizador transversal de links --------

def _sanitize_links_in_blocks(blocks: list[dict]) -> list[dict]:
    blks = json.loads(json.dumps(blocks))
    def strip_or_fix_rt(rt_list):
        for tkn in rt_list:
            if tkn.get("type") == "text":
                link = tkn.get("text", {}).get("link")
                if link and "url" in link:
                    safe = _clean_url(link["url"])
                    if not safe:
                        tkn["text"]["link"] = None
                    else:
                        tkn["text"]["link"]["url"] = safe
        return rt_list
    for b in blks:
        t = b.get("type")
        if t in ("paragraph","bulleted_list_item","to_do","heading_1","heading_2","heading_3","callout"):
            if t in b and "rich_text" in b[t]:
                b[t]["rich_text"] = strip_or_fix_rt(b[t]["rich_text"])
        elif t == "table":
            for row in b.get("table", {}).get("children", []):
                cells = row.get("table_row", {}).get("cells", [])
                for i, cell in enumerate(cells):
                    cells[i] = strip_or_fix_rt(cell)
        elif t == "image":
            img = b.get("image", {})
            if img.get("type") == "external" and "external" in img and "url" in img["external"]:
                safe = _clean_url(img["external"]["url"])
                if not safe:
                    caption = ""
                    cap_rt = img.get("caption") or []
                    if cap_rt and cap_rt[0].get("type") == "text":
                        caption = cap_rt[0]["text"].get("content","")
                    b.clear(); b.update(_para(caption or ""))
                else:
                    img["external"]["url"] = safe
            cap = img.get("caption", [])
            if cap:
                b["image"]["caption"] = strip_or_fix_rt(cap)
    return blks

# ---------- KPIs UI helpers ----------

def _metric_card(title: str, value: str, sub: str = "", icon: str = "üìä") -> dict:
    text = f"{title}: {value}" if not sub else f"{title}: {value}\n{sub}"
    return _callout(text, icon=icon)

def build_kpi_section_blocks(kpis: dict | None, total_items: int) -> list[dict]:
    blocks = []
    blocks.append(_h2("KPIs a la vista"))
    if not kpis:
        blocks.append(_para("‚Äî"))
        return blocks

    tickets = str(kpis.get("tickets_resueltos","‚Äî"))
    pct_val = kpis.get("tasa_1ra_respuesta", None)
    pct = f"{pct_val:.0f}%" if isinstance(pct_val, (int,float)) else "‚Äî"
    base = kpis.get("first_base", 0); ok = kpis.get("first_ok", 0)
    ttr_mean = _fmt_h(kpis.get("ttr_horas"))
    ttr_p50  = _fmt_h(kpis.get("ttr_p50_h"))
    ttr_p90  = _fmt_h(kpis.get("ttr_p90_h"))
    csat = kpis.get("csat", None); csn = kpis.get("csat_n",0)
    csat_txt = "‚Äî" if csat is None else f"{csat:.2f}"
    sla = kpis.get("sla_minutes", 15)

    row1 = [
        [_metric_card("Tickets resueltos", tickets, f"Total convers. procesadas: {total_items}", "üéüÔ∏è")],
        [_metric_card("% 1ra respuesta en SLA", pct, f"SLA {sla} min ‚Ä¢ {ok}/{base}", "‚ö°")],
        [_metric_card("TTR medio", ttr_mean, f"p50 {ttr_p50} ‚Ä¢ p90 {ttr_p90}", "‚è±Ô∏è")],
        [_metric_card("CSAT", csat_txt, f"n={csn}", "‚≠ê")],
    ]
    blocks.append(_column_list([c for c in row1]))
    return blocks

def build_kpi_glossary_blocks(kpis: dict | None, total_items: int) -> list[dict]:
    if not kpis:
        return []
    pct_val = kpis.get("tasa_1ra_respuesta", None)
    pct = f"{pct_val:.0f}%" if isinstance(pct_val, (int,float)) else "‚Äî"
    base = kpis.get("first_base", 0); ok = kpis.get("first_ok", 0)
    med_first = _fmt_min_from_sec(kpis.get("first_resp_p50_s"))
    ttr_mean = _fmt_h(kpis.get("ttr_horas"))
    ttr_p50  = _fmt_h(kpis.get("ttr_p50_h"))
    ttr_p90  = _fmt_h(kpis.get("ttr_p90_h"))
    csat = kpis.get("csat", None); csn = kpis.get("csat_n",0)
    csat_txt = "‚Äî" if csat is None else f"{csat:.2f}"
    blocks = []
    blocks.append(_h2("C√≥mo leer los KPIs"))
    # Tickets resueltos
    blocks.append(_h3("üéüÔ∏è Tickets resueltos"))
    blocks.append(_bullet(f"**{kpis.get('tickets_resueltos','‚Äî')}** ‚Üí son las conversaciones que efectivamente se cerraron con resoluci√≥n."))
    blocks.append(_bullet(f"**{total_items} procesadas** ‚Üí es el total de conversaciones analizadas (algunas quedaron abiertas, duplicadas o fuera de alcance)."))
    blocks.append(_para("üëâ Este n√∫mero muestra el **volumen de trabajo resuelto** frente al total que entr√≥."))
    # % primera respuesta
    blocks.append(_h3("‚ö° % 1ra respuesta en SLA"))
    blocks.append(_bullet(f"**{pct} ({ok}/{base})** ‚Üí conversaciones con **primera respuesta** dentro del SLA."))
    blocks.append(_bullet(f"**SLA {kpis.get('sla_minutes',15)} min** ‚Üí compromiso de contestar dentro de 15 minutos."))
    blocks.append(_bullet(f"**Mediana {med_first}** ‚Üí la mayor√≠a de las veces la primera respuesta fue **inmediata**."))
    blocks.append(_para("üëâ Este KPI mide la **velocidad de reacci√≥n inicial** del equipo."))
    # TTR
    blocks.append(_h3("‚è±Ô∏è TTR (Time To Resolution)"))
    blocks.append(_bullet(f"**Promedio: {ttr_mean}** ‚Üí tiempo promedio hasta la resoluci√≥n."))
    blocks.append(_bullet(f"**p50 ({ttr_p50})** ‚Üí 50% de los tickets se resuelven por debajo de la mediana."))
    blocks.append(_bullet(f"**p90 ({ttr_p90})** ‚Üí 90% se resuelve por debajo de este valor."))
    blocks.append(_para("üëâ El TTR muestra la **distribuci√≥n de los tiempos**: la mayor√≠a se resuelve r√°pido, pero algunos casos largos suben el promedio."))
    # CSAT
    blocks.append(_h3("‚≠ê CSAT (Customer Satisfaction)"))
    blocks.append(_bullet(f"**{csat_txt} (n={csn})** ‚Üí promedio de satisfacci√≥n de clientes (sobre 5)."))
    blocks.append(_bullet(f"**n={csn}** ‚Üí cantidad de respuestas v√°lidas."))
    blocks.append(_para("üëâ Refleja la **percepci√≥n del cliente**. Un valor < 3 indica **espacio de mejora**."))
    return blocks

# ===================== P√°gina Notion =====================

def notion_create_page(parent_page_id: str,
                       token: str,
                       page_title: str,
                       df: pd.DataFrame,
                       resumen_df: pd.DataFrame,
                       meta: dict,
                       chart_urls: dict,
                       insights: dict,
                       acciones_ai: dict | None = None,
                       kpis: dict | None = None,
                       top_counts_series: pd.Series | None = None):
    blocks = []
    blocks.append(_h1(page_title))

    # Resumen
    blocks.append(_h2("Resumen Ejecutivo"))
    blocks.append(_para(f"üìÖ Fecha del an√°lisis: {meta.get('fecha','')}"))
    blocks.append(_para(f"üìÇ Fuente de datos: {meta.get('fuente','')}"))
    blocks.append(_para(f"üí¨ Conversaciones procesadas: {meta.get('total','')}"))
    blocks.append(_para("Durante el periodo analizado se registraron conversaciones en Intercom, procesadas por IA para identificar patrones, problemas recurrentes y oportunidades de mejora."))
    blocks.append(_callout("Foco de la semana: reducir TTR p50 en Top-2 issues (-20% en 14 d√≠as) y +0.2 CSAT.", icon="üéØ"))

    # KPIs (cards) + glosario
    blocks.extend(build_kpi_section_blocks(kpis, total_items=len(df)))
    blocks.append(_divider())
    blocks.extend(build_kpi_glossary_blocks(kpis, total_items=len(df)))

    # Top 3 issues (usa la MISMA serie que el gr√°fico)
    blocks.append(_divider())
    blocks.append(_h2("Top 3 issues"))
    total_len = max(len(df), 1)
    series = (top_counts_series if top_counts_series is not None else df["issue_group"].value_counts())
    top3_rows = resumen_df.sort_values("casos", ascending=False).head(3)
    if len(top3_rows) == 0:
        blocks.append(_para("‚Äî"))
    else:
        total_len = max(len(df), 1)
        for _, r in top3_rows.iterrows():
            issue = str(r.get("issue",""))
            casos = int(r.get("casos",0) or 0)
            pct_issue = f"{(casos/total_len*100):.0f}%"
            blocks.append(_bullet(f"{issue} ‚Üí {casos} casos ({pct_issue})"))

    # Gr√°ficos + insights (siempre DEBAJO de cada imagen)
    for key, caption in [
        ("top_issues", "Top Issues"),
        ("urgencia_pie", "Distribuci√≥n de Urgencias"),
        ("sentimiento_pie", "Distribuci√≥n de Sentimientos"),
        ("urgencia_por_issue", "Urgencia por Issue"),
        ("urgencia_top_issues", "Urgencias en Top Issues (agrupadas)"),
        ("canal_por_issue", "Canal por Issue"),
        ("ttr_por_urgencia_box", "TTR por Urgencia (horas)"),
    ]:
        url = (chart_urls or {}).get(key)
        blk = _image_external_if_valid(url, caption)
        if blk:
            blocks.append(blk)
        if insights.get(key):
            blocks.append(_callout(insights[key], icon="üí°"))

    # Categorizaciones manuales
    blocks.append(_h2("Categorizaciones manuales"))

    # Tema
    blocks.append(_h3("Tema"))
    tema_series = df["tema_norm"].fillna("").replace({"nan": ""}).astype(str).str.strip()
    tema_vc = tema_series[tema_series != ""].value_counts().head(5)
    if len(tema_vc) == 0:
        blocks.append(_para("‚Äî"))
    else:
        for k, v in tema_vc.items():
            blocks.append(_bullet(f"{k}: {v}"))

    # Motivo
    blocks.append(_h3("Motivo"))
    motivo_series = df["motivo_norm"].fillna("").replace({"nan": ""}).astype(str).str.strip()
    motivo_vc = motivo_series[motivo_series != ""].value_counts().head(5)
    if len(motivo_vc) == 0:
        blocks.append(_para("‚Äî"))
    else:
        for k, v in motivo_vc.items():
            blocks.append(_bullet(f"{k}: {v}"))

    # Issues Detallados (tabla)
    blocks.append(_h2("Issues Detallados"))
    blocks.append(build_issues_table_block(resumen_df))

    # Acciones a evaluar
    blocks.extend(build_actions_section_blocks(resumen_df, top_n=5, acciones_ai=acciones_ai))

    # Sanitizado final de enlaces
    safe_children = _sanitize_links_in_blocks(blocks)

    payload = {
        "parent": {"type": "page_id", "page_id": parent_page_id},
        "properties": {"title": {"title": [{"text": {"content": page_title}}]}},
        "children": safe_children
    }

    resp = requests.post(
        "https://api.notion.com/v1/pages",
        headers={
            "Authorization": f"Bearer {token}",
            "Notion-Version": "2022-06-28",
            "Content-Type": "application/json"
        },
        data=json.dumps(payload)
    )

    # Retry sin links si Notion protesta por URLs inv√°lidas
    if not resp.ok and ("Invalid URL" in (resp.text or "") or "link" in (resp.text or "").lower()):
        def strip_all_links(blks: list[dict]) -> list[dict]:
            blks = json.loads(json.dumps(blks))
            for b in blks:
                t = b.get("type")
                if t in ("paragraph", "bulleted_list_item", "to_do", "heading_1", "heading_2", "heading_3", "callout"):
                    rt = b.get(t, {}).get("rich_text", [])
                    for tkn in rt:
                        if tkn.get("type") == "text" and tkn.get("text", {}).get("link"):
                            tkn["text"]["link"] = None
                elif t == "table":
                    for row in b.get("table", {}).get("children", []):
                        cells = row.get("table_row", {}).get("cells", [])
                        for cell in cells:
                            for tkn in cell:
                                if tkn.get("type") == "text" and tkn.get("text", {}).get("link"):
                                    tkn["text"]["link"] = None
                elif t == "image":
                    cap = b.get("image", {}).get("caption", [])
                    for tkn in cap:
                        if tkn.get("type") == "text" and tkn.get("text", {}).get("link"):
                            tkn["text"]["link"] = None
            return blks

        payload_retry = dict(payload)
        payload_retry["children"] = strip_all_links(payload["children"])
        resp = requests.post(
            "https://api.notion.com/v1/pages",
            headers={
                "Authorization": f"Bearer {token}",
                "Notion-Version": "2022-06-28",
                "Content-Type": "application/json"
            },
            data=json.dumps(payload_retry)
        )

    if not resp.ok:
        raise RuntimeError(f"Notion error {resp.status_code}: {resp.text}")
    return resp.json()

# ===================== Core =====================

def _fallback_insights(insights: dict,
                       top_counts: pd.Series,
                       urg_counts: pd.Series,
                       sent_counts: pd.Series,
                       ttr_urg_stats: dict):
    """Genera insights determin√≠sticos cuando no hay IA, incluyendo el foquito de TTR por urgencia."""
    if "top_issues" not in insights and len(top_counts):
        issue0, n0 = top_counts.index[0], int(top_counts.iloc[0])
        insights["top_issues"] = f"El principal problema es {issue0} ({n0} casos). Priorizar fixes para reducir volumen y TTR."

    if "urgencia_pie" not in insights and len(urg_counts):
        alta = int(urg_counts.get("Alta", 0)); media = int(urg_counts.get("Media", 0))
        tot = int(urg_counts.sum() or 1)
        insights["urgencia_pie"] = f"El {round(100*(alta+media)/tot)}% de los tickets son Alta/Media. Asegurar priorizaci√≥n y SLAs."

    if "sentimiento_pie" not in insights and len(sent_counts):
        neg = int(sent_counts.get("Negativo", 0)); tot = int(sent_counts.sum() or 1)
        if neg:
            insights["sentimiento_pie"] = f"El {round(100*neg/tot)}% de los tickets reflejan sentimiento negativo. Revisar comunicaci√≥n y tiempos."

    if "ttr_por_urgencia_box" not in insights and ttr_urg_stats:
        def fmt(h): 
            return f"{h:.1f} h" if isinstance(h, (int,float)) else "‚Äî"
        p50_alt = ttr_urg_stats.get("Alta",{}).get("p50")
        p50_med = ttr_urg_stats.get("Media",{}).get("p50")
        p50_baj = ttr_urg_stats.get("Baja",{}).get("p50")
        insights["ttr_por_urgencia_box"] = (
            f"TTR mediano ‚Äî Alta: {fmt(p50_alt)}, Media: {fmt(p50_med)}, Baja: {fmt(p50_baj)}. "
            f"Objetivo: bajar p50 en 'Media' (y mantener 'Alta' < 'Media')."
        )
    return insights

def run(csv_path: str,
        out_dir: str,
        notion_token: str | None,
        notion_parent: str | None,
        publish_github: bool = False,
        github_repo_path: str | None = None,
        github_branch: str = "main",
        assets_base_url: str | None = None,
        gemini_api_key: str | None = None,
        gemini_model: str = "gemini-1.5-flash",
        sla_first_reply_min: int = 15,
        ai_mode: str = "full",
        ai_budget: int = 100):

    os.makedirs(out_dir, exist_ok=True)

    # Lectura + normalizaci√≥n + taxonom√≠as
    df = load_csv_robusto(csv_path)
    df = normalize_columns(df)
    for col in ["tema","motivo","submotivo","urgencia","canal","area","sentimiento","categoria"]:
        if col in df.columns:
            df[col] = df[col].astype(object)
    df = enforce_taxonomy(df)

    # Consolidaciones
    df["csat_num"] = df.apply(pick_csats, axis=1)
    if "csat" in df.columns:
        df["csat"] = df["csat"].where(pd.to_numeric(df["csat"], errors="coerce").between(1,5), df["csat_num"])
    else:
        df["csat"] = df["csat_num"]

    if "texto_base" not in df.columns:
        df["texto_base"] = df.apply(build_text_base, axis=1)
    else:
        df["texto_base"] = df["texto_base"].fillna("")

    df["estado_final"] = df.apply(estado_final_from, axis=1)

    for c in ["motivo_norm", "submotivo_norm", "texto_base"]:
        if c in df.columns:
            df[c] = df[c].astype(object).where(~pd.isna(df[c]), "")

    df["issue_group"] = df.apply(
        lambda r: choose_issue(r.get("motivo_norm"), r.get("submotivo_norm"), r.get("texto_base")),
        axis=1
    )

    # Quality gate
    df["resumen_qc"] = df.apply(lambda r: r["resumen_ia"] if not _is_vague_summary(r.get("resumen_ia","")) else _enrich_summary(r), axis=1)
    df["insight_qc"] = df.apply(lambda r: r["insight_ia"] if not _is_vague_insight(r.get("insight_ia","")) else _enrich_insight(r), axis=1)

    # Anti "tr√≠o vago" (solo interno; no se imprime en Notion)
    mask_trio = (df["sentimiento"].astype(str).str.lower().eq("neutro")) & \
                (df["urgencia"].astype(str).str.lower().eq("media")) & \
                (df["csat"].astype(str).eq("3"))
    df.loc[mask_trio, "sentimiento"] = df.loc[mask_trio, "texto_base"].map(_sent_rule)
    df.loc[mask_trio, "urgencia"]    = df.loc[mask_trio, "texto_base"].map(_urg_rule)
    df.loc[mask_trio, "csat"]        = df.loc[mask_trio].apply(lambda r: _csat_rule(r["sentimiento"], r["estado_final"]), axis=1)
    triovago_rate = round(100 * mask_trio.mean(), 1)

    # Heur√≠stica de silencio => resuelto
    df["estado_final"] = df.apply(_assume_resuelto_por_silencio, axis=1)

    # KPI desde crudos (+ guarda frs/ttr al df)
    _gemini_smoke_test(gemini_api_key, gemini_model)
    kpis = compute_kpis_from_raw_df(df, sla_minutes=sla_first_reply_min)
    if "estado_final" in df.columns:
        kpis["tickets_resueltos"] = int((df["estado_final"] == "Resuelto").sum())

    # Flags
    flags = df.apply(compute_flags, axis=1)
    for c in flags.columns:
        df[c] = flags[c]

    # Resumen por issue
    rows = []
    for issue, grp in df.groupby("issue_group"):
        canales = top_values(grp["canal"])
        areas = top_values(grp["area"])
        motivos = top_values(grp["motivo_norm"])
        submotivos = top_values(grp["submotivo_norm"])
        ejemplos = grp.loc[grp["link_a_intercom"].astype(str).str.len() > 0, "link_a_intercom"].head(3).astype(str).tolist()
        rows.append({
            "issue": issue,
            "casos": int(len(grp)),
            "canales_top": canales,
            "areas_top": areas,
            "motivos_top": motivos,
            "submotivos_top": submotivos,
            "ejemplos_intercom": " | ".join(ejemplos)
        })
    resumen_df = pd.DataFrame(rows).sort_values("casos", ascending=False)
    resumen_df = compare_with_prev(resumen_df, hist_dir=os.path.join(out_dir, "hist"))

    # Exports CSV
    issues_csv = os.path.join(out_dir, "issues_resumen.csv")
    casos_csv = os.path.join(out_dir, "casos_con_issue.csv")
    df_export_cols = ["fecha","canal","rol","area","tema_norm","motivo_norm","submotivo_norm",
                      "categoria","urgencia","sentimiento","resumen_ia","insight_ia","resumen_qc","insight_qc",
                      "palabras_clave","issue_group","estado_final","risk","sla_now","taxonomy_flag",
                      "link_a_intercom","id_intercom",
                      "created_at","first_admin_reply_at","first_contact_reply_at","closed_at",
                      "first_response_seconds","ttr_seconds","status","csat","csat_num"]
    existing = [c for c in df_export_cols if c in df.columns]
    df[existing].to_csv(casos_csv, index=False, encoding="utf-8")
    resumen_df.to_csv(issues_csv, index=False, encoding="utf-8")

    total = len(df)

    # Gr√°ficos
    p_top,   all_counts = chart_top_issues(df, out_dir)   # serie completa
    p_urg_pie, urg_counts = chart_urgencia_pie(df, out_dir)
    p_sent_pie, sent_counts = chart_sentimiento_pie(df, out_dir)
    p_urg_issue, urg_issue_ct = chart_urgencia_por_issue(df, out_dir)
    p_canal_issue, canal_issue_ct = chart_canal_por_issue(df, out_dir)
    p_urg_top, urg_top_ct = chart_urgencias_en_top_issues(df, out_dir)
    p_ttr_urg, ttr_urg_stats = chart_ttr_box_by_urgencia(df, out_dir)

    # Assets p√∫blicos
    if publish_github and github_repo_path:
        try:
            assets_base_url = publish_images_to_github(
                out_dir=out_dir,
                repo_path=github_repo_path,
                branch=github_branch,
                date_subdir=date.today().isoformat(),
                files=[
                    os.path.basename(p_top),
                    os.path.basename(p_urg_pie),
                    os.path.basename(p_sent_pie),
                    os.path.basename(p_urg_issue),
                    os.path.basename(p_canal_issue),
                    os.path.basename(p_urg_top),
                    os.path.basename(p_ttr_urg),
                ],
            )
            print(f"üåê Assets publicados en: {assets_base_url}")
        except Exception as e:
            print(f"‚ö†Ô∏è No pude publicar en GitHub: {e}")

    chart_urls = {}
    if assets_base_url:
        chart_urls = {
            "top_issues": f"{assets_base_url}/{os.path.basename(p_top)}",
            "urgencia_pie": f"{assets_base_url}/{os.path.basename(p_urg_pie)}",
            "sentimiento_pie": f"{assets_base_url}/{os.path.basename(p_sent_pie)}",
            "urgencia_por_issue": f"{assets_base_url}/{os.path.basename(p_urg_issue)}",
            "canal_por_issue": f"{assets_base_url}/{os.path.basename(p_canal_issue)}",
            "urgencia_top_issues": f"{assets_base_url}/{os.path.basename(p_urg_top)}",
            "ttr_por_urgencia_box": f"{assets_base_url}/{os.path.basename(p_ttr_urg)}",
        }

    # ===== IA: modos, presupuesto y cache =====
    mode = (ai_mode or "full").strip().lower()
    if mode == "off":
        budget = 0
    elif mode == "lite":
        budget = min(ai_budget, 6) if ai_budget else 6
    else:
        budget = ai_budget if ai_budget else 100

    ai = _AIBudget(budget)
    cache_path = os.path.join(out_dir, "hist", "ai_cache.json")
    cache = _load_ai_cache(cache_path)
    fp = _dataset_fingerprint(df)

    insights = {}
    acciones_ai = {}

    if fp in cache:
        cached = cache[fp]
        insights = cached.get("insights", {})
        acciones_ai = cached.get("acciones_ai", {})
        print(f"‚ôªÔ∏è Reutilizando insights desde cache. insights={len(insights)} | acciones={len(acciones_ai)}")
    elif budget > 0 and (gemini_api_key or os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")):
        def try_insight(name, obj):
            nonlocal insights
            if not ai.take():
                return
            txt = ai_insight_for_chart(name, obj, api_key=gemini_api_key, model=gemini_model)
            if txt == "__RATE_LIMIT__":
                ai.rate_limited = True
                return
            if txt:
                insights[name] = txt

        try_insight("top_issues", all_counts.to_dict())
        try_insight("urgencia_pie", urg_counts.to_dict())
        try_insight("sentimiento_pie", sent_counts.to_dict())
        if not ai.rate_limited:
            try_insight("urgencia_por_issue", urg_issue_ct)
        if not ai.rate_limited:
            try_insight("canal_por_issue", canal_issue_ct)
        if not ai.rate_limited:
            try_insight("urgencia_top_issues", urg_top_ct)
        if not ai.rate_limited:
            try_insight("ttr_por_urgencia_box", ttr_urg_stats)

        # Acciones por issue (top-N)
        max_actions = 5 if mode == "full" else 2 if mode == "lite" else 0
        for _, row in resumen_df.head(max_actions).iterrows():
            if ai.rate_limited or not ai.take():
                break
            issue = str(row.get("issue",""))
            ctx = {
                "canales_top": str(row.get("canales_top","")),
                "areas_top": str(row.get("areas_top","")),
                "motivos_top": str(row.get("motivos_top","")),
                "submotivos_top": str(row.get("submotivos_top","")),
                "total_casos_issue": int(row.get("casos",0) or 0),
                "total_casos_semana": total
            }
            add = ai_actions_for_issue(issue, ctx, api_key=gemini_api_key, model=gemini_model)
            if add.get("__rate_limited__"):
                ai.rate_limited = True
                break
            if add:
                acciones_ai[issue] = add

        if insights or acciones_ai:
            cache[fp] = {"insights": insights, "acciones_ai": acciones_ai}
            _save_ai_cache(cache_path, cache)

    # Fallbacks determin√≠sticos (garantiza foquito en TTR por urgencia)
    insights = _fallback_insights(insights, all_counts, urg_counts, sent_counts, ttr_urg_stats)

    used = budget - ai.budget
    print(f"ü§ñ IA ‚Üí modo={mode} | usadas={used}/{budget} | rate_limited={ai.rate_limited} | insights={len(insights)} | acciones={len(acciones_ai)}")
    # M√©tricas internas (no p√∫blicas)
    print(json.dumps({"triovago_rate_%": triovago_rate}, ensure_ascii=False))

    # Notion
    if notion_token and notion_parent:
        page_title = f"Reporte CRM ‚Äì {date.today().isoformat()}"
        meta = {
            "fecha": date.today().isoformat(),
            "fuente": os.path.basename(csv_path),
            "total": total,
        }
        page = notion_create_page(
            parent_page_id=notion_parent,
            token=notion_token,
            page_title=page_title,
            df=df,
            resumen_df=resumen_df,
            meta=meta,
            chart_urls=chart_urls,
            insights=insights,
            acciones_ai=acciones_ai if acciones_ai else None,
            kpis=kpis,
            top_counts_series=all_counts
        )
        print(f"‚úÖ Publicado en Notion: {page.get('url','(sin url)')}")
    else:
        print("‚ÑπÔ∏è Notion no configurado. Se generaron archivos locales.")
        print(json.dumps({
            "issues_csv": issues_csv,
            "casos_csv": casos_csv,
            "charts": {
                "top_issues": p_top,
                "urgencia_pie": p_urg_pie,
                "sentimiento_pie": p_sent_pie,
                "urgencia_por_issue": p_urg_issue,
                "canal_por_issue": p_canal_issue,
                "urgencia_top_issues": p_urg_top,
                "ttr_por_urgencia_box": p_ttr_urg
            },
            "insights": insights
        }, indent=2, ensure_ascii=False))

# ===================== CLI =====================

def main():
    ap = argparse.ArgumentParser(description="Venti ‚Äì Insight IA (Notion narrativo + Gemini API)")
    ap.add_argument("--csv", required=True, help="Ruta al CSV de conversaciones")
    ap.add_argument("--out", default="./salida", help="Directorio de salida")
    ap.add_argument("--notion_token", default=os.getenv("NOTION_TOKEN"), help="Token de Notion")
    ap.add_argument("--notion_parent", default=os.getenv("NOTION_PARENT_PAGE_ID"), help="ID de p√°gina padre en Notion")
    ap.add_argument("--publish_github", action="store_true", help="Publicar PNGs al repo y usar URL p√∫blica")
    ap.add_argument("--github_repo_path", default=None, help="Ruta local al repo clonado")
    ap.add_argument("--github_branch", default="main", help="Branch destino")
    ap.add_argument("--assets_base_url", default=None, help="URL base p√∫blica ya hosteada (si no publicas a GitHub)")
    ap.add_argument("--gemini_api_key", default=os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY"), help="API key de Gemini")
    ap.add_argument("--gemini_model", default="gemini-1.5-flash", help="Modelo de Gemini")
    ap.add_argument("--sla_first_reply_min", type=int, default=15, help="SLA (min) para % 1ra respuesta")
    ap.add_argument("--ai_mode", choices=["full","lite","off"], default="full", help="Nivel de generaci√≥n con IA")
    ap.add_argument("--ai_budget", type=int, default=100, help="M√°ximo de requests a Gemini por corrida")
    args = ap.parse_args()

    print("‚ñ∂ Script iniciado")
    print("CSV:", args.csv)
    print("OUT:", args.out)
    print("NOTION_TOKEN:", "OK" if args.notion_token else "FALTA")
    print("PARENT:", args.notion_parent)

    run(
        csv_path=args.csv,
        out_dir=args.out,
        notion_token=args.notion_token,
        notion_parent=args.notion_parent,
        publish_github=args.publish_github,
        github_repo_path=args.github_repo_path,
        github_branch=args.github_branch,
        assets_base_url=args.assets_base_url,
        gemini_api_key=args.gemini_api_key,
        gemini_model=args.gemini_model,
        sla_first_reply_min=args.sla_first_reply_min,
        ai_mode=args.ai_mode,
        ai_budget=args.ai_budget
    )

if __name__ == "__main__":
    main()
